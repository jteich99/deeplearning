{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNrfPQ7Jf7sXvbfwlfKRU1r"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# MSE: Mean Squared Error\n","\n","## Definición\n","El **MSE** (Error Cuadrático Medio) mide el error promedio cuadrático entre los valores reales y las predicciones.\n","\n","$$\n","\\text{MSE}=\\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\hat y_i)^2\n","$$\n","\n","- $n$: número de muestras.  \n","- $y_i$: valor real de la muestra $i$.  \n","- $\\hat y_i$: valor predicho para la muestra $i$.  \n","- $(y_i-\\hat y_i)$: residuo o error de predicción en la muestra $i$.\n","\n","## Dataset del ejemplo\n","Contexto: problema de **regresión**, donde se predice una variable continua.  \n","\n","Variables:\n","- $y$: vector con los valores reales observados.  \n","- $\\hat y$: vector con las predicciones del modelo.  \n","- $r$: vector de residuos de la resta $y_i - \\hat y_i$.  \n","\n","Muestras:\n","| $i$ | $y_i$  | $\\hat y_i$  | $r_i$ | $r_i^2$ |\n","|---|------|------|--------------------------|-------|\n","| 1 | 3.0  | 2.5  | 0.5                      | 0.25  |\n","| 2 | -0.5 | 0.0  | -0.5                     | 0.25  |\n","| 3 | 2.0  | 2.0  | 0.0                      | 0.00  |\n","| 4 | 7.0  | 8.0  | -1.0                     | 1.00  |\n","| 5 | 4.5  | 5.0  | -0.5                     | 0.25  |\n","\n","Suma de cuadrados:  \n","$$0.25 + 0.25 + 0.00 + 1.00 + 0.25 = 1.75$$  \n","\n","Promedio:  \n","$$\\text{MSE} = \\frac{1.75}{5} = 0.35$$  \n","\n","**Interpretación:**  \n","El valor medio del cuadrado de los errores es **0.35**.  \n","Esto indica que, en promedio, las predicciones se desvían del valor real en una magnitud que al cuadrarse equivale a 0.35.  \n","El MSE **penaliza más los errores grandes** debido a la operación cuadrática.\n"],"metadata":{"id":"kXOkuzbI1qnq"}},{"cell_type":"markdown","source":["---\n","# BCE: Binary Cross-Entropy\n","\n","## Definición\n","El **BCE** mide el desacuerdo entre las probabilidades predichas y las etiquetas reales en **clasificación binaria**.\n","\n","$$\n","\\text{BCE}=-\\frac{1}{n}\\sum_{i=1}^{n}\\left[y_i\\ln(\\hat y_i)+(1-y_i)\\ln(1-\\hat y_i)\\right]\n","$$\n","\n","- $n$: número de muestras.\n","- $y_i \\in \\{0,1\\}$: etiqueta real binaria.\n","- $\\hat y_i \\in (0,1)$: probabilidad predicha de clase positiva.\n","- Si $y_i=1$ aporta $-\\ln(\\hat y_i)$.\n","- Si $y_i=0$ aporta $-\\ln(1-\\hat y_i)$.\n","\n","## Dataset del ejemplo\n","Contexto: un modelo con salida sigmoide que predice la probabilidad de clase positiva.  \n","Variables:\n","- $y$: vector de etiquetas verdaderas.\n","- $\\hat y$: vector de probabilidades predichas para la clase 1.\n","\n","Muestras:\n","| $i$ | $y_i$ | $\\hat y_i$  | ${BCE}_i$ |\n","|---|-----|------|----------------|\n","| 1 | 1   | 0.9  | $-\\ln(0.9)=0.1053605$ |\n","| 2 | 0   | 0.2  | $-\\ln(1-0.2)=0.2231436$ |\n","| 3 | 1   | 0.4  | $-\\ln(0.4)=0.9162907$ |\n","| 4 | 0   | 0.8  | $-\\ln(1-0.8)=1.6094379$ |\n","| 5 | 1   | 0.7  | $-\\ln(0.7)=0.3566749$ |\n","\n","Promedio:\n","$$\n","\\text{BCE}=\\frac{0.1053605+0.2231436+0.9162907+1.6094379+0.3566749}{5}\\approx 0.6421815\n","$$\n","\n","Interpretación: penaliza fuertemente estar muy seguro y equivocado. Ideal con función de activación **sigmoide en la capa de salida**.\n"],"metadata":{"id":"hzinV5t52okG"}},{"cell_type":"markdown","source":["---\n","# CE multiclase: Categorical Cross-Entropy\n","\n","## Definición\n","La **entropía cruzada multiclase** mide el desacuerdo entre la distribución real y la distribución predicha en **clasificación con C clases**.\n","\n","$$\n","\\text{CE}=-\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{c=1}^{C}y_{i,c}\\ln(\\hat y_{i,c})\n","$$\n","\n","- $n$: número de muestras.\n","- $C$: número de clases.\n","- $y_{i,c}$: 1 si la muestra $i$ pertenece a la clase $c$, 0 en caso contrario (one hot).\n","- $\\hat y_{i,c}$: probabilidad predicha de la clase $c$ para la muestra $i$ (salida softmax).\n","- Con one hot, para cada muestra solo cuenta la probabilidad de la clase correcta.\n","\n","---\n","\n","## Dataset del ejemplo\n","Contexto: un clasificador de 3 clases con salida softmax.  \n","Variables:\n","- $y$: matriz one hot de etiquetas reales.\n","- $\\hat y$: matriz de probabilidades predichas por clase.\n","\n","Muestras:\n","| $i$ | $y_i$ (one hot) | $\\hat y_i$ (probabilidades) |\n","|---|----------------|----------------------|\n","| 1 | [1, 0, 0] | [0.70, 0.20, 0.10] |\n","| 2 | [0, 1, 0] | [0.10, 0.60, 0.30] |\n","| 3 | [0, 0, 1] | [0.20, 0.10, 0.70] |\n","| 4 | [1, 0, 0] | [0.85, 0.10, 0.05] |\n","| 5 | [0, 0, 1] | [0.05, 0.25, 0.70] |\n","\n","Término por muestra usando la clase correcta:\n","| $i$ | Clase verdadera | Prob. predicha correcta | ${CE}_i$ |\n","|---|------------------|-------------------------|----------------------------|\n","| 1 | 1 | 0.70 | 0.3566749 |\n","| 2 | 2 | 0.60 | 0.5108256 |\n","| 3 | 3 | 0.70 | 0.3566749 |\n","| 4 | 1 | 0.85 | 0.1625189 |\n","| 5 | 3 | 0.70 | 0.3566749 |\n","\n","Promedio:\n","$$\n","\\text{CE}=\\frac{0.3566749+0.5108256+0.3566749+0.1625189+0.3566749}{5}\\approx 0.3486738\n","$$\n","\n","Interpretación: si la probabilidad de la clase correcta es alta, la pérdida es baja. Se usa con función de activación **softmax** en la salida.\n"],"metadata":{"id":"0EIGS22b3LKZ"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gqNyMKVe1OCo","executionInfo":{"status":"ok","timestamp":1761824080116,"user_tz":300,"elapsed":26,"user":{"displayName":"Gerardo Alexis Vilcamiza Espinoza","userId":"15529282681696955571"}},"outputId":"c6d78e50-d6f6-4125-a405-b27ad23c69c4"},"outputs":[{"output_type":"stream","name":"stdout","text":["=== MSE (Error Cuadrático Medio) ===\n","i |   y_i |  ŷ_i  | residuo | residuo^2\n","1 |   3.00 |   2.50 |    0.500 |    0.2500\n","2 |  -0.50 |   0.00 |   -0.500 |    0.2500\n","3 |   2.00 |   2.00 |    0.000 |    0.0000\n","4 |   7.00 |   8.00 |   -1.000 |    1.0000\n","5 |   4.50 |   5.00 |   -0.500 |    0.2500\n","MSE promedio = 0.35\n","\n","=== BCE (Entropía Cruzada Binaria) ===\n","i | y_i | ŷ_i  | -[y_i ln(ŷ_i) + (1 - y_i) ln(1 - ŷ_i)]\n","1 | 1   |  0.900 |   0.105361\n","2 | 0   |  0.200 |   0.223144\n","3 | 1   |  0.400 |   0.916291\n","4 | 0   |  0.800 |   1.609438\n","5 | 1   |  0.700 |   0.356675\n","BCE promedio = 0.6421815310438047\n","\n","=== CE (Entropía Cruzada Multiclase) ===\n","i | clase | prob_correcta | -ln(prob_correcta)\n","1 |      1 |          0.700 |    0.356674944\n","2 |      2 |          0.600 |    0.510825624\n","3 |      3 |          0.700 |    0.356674944\n","4 |      1 |          0.850 |    0.162518929\n","5 |      3 |          0.700 |    0.356674944\n","CE promedio = 0.3486738770159926\n","\n"]}],"source":["import numpy as np\n","\n","# ------------------------------------------------\n","# MSE: Mean Squared Error\n","# ------------------------------------------------\n","y_mse = np.array([3.0, -0.5, 2.0, 7.0, 4.5], dtype=float)\n","yhat_mse = np.array([2.5, 0.0, 2.0, 8.0, 5.0], dtype=float)\n","\n","res = y_mse - yhat_mse\n","mse = np.mean(res**2)\n","\n","print(\"=== MSE (Error Cuadrático Medio) ===\")\n","print(\"i |   y_i |  ŷ_i  | residuo | residuo^2\")\n","for i, (yi, yhi, ri) in enumerate(zip(y_mse, yhat_mse, res), start=1):\n","    print(f\"{i:1d} | {yi:6.2f} | {yhi:6.2f} | {ri:8.3f} | {ri**2:9.4f}\")\n","print(\"MSE promedio =\", mse)\n","print()\n","\n","# ------------------------------------------------\n","# BCE: Binary Cross-Entropy\n","# ------------------------------------------------\n","y_bce = np.array([1, 0, 1, 0, 1], dtype=float)\n","yhat_bce = np.array([0.9, 0.2, 0.4, 0.8, 0.7], dtype=float)\n","\n","# Clipping para evitar log(0)\n","eps = 1e-12\n","yhat_bce = np.clip(yhat_bce, eps, 1 - eps)\n","\n","# Uso de logaritmo natural (ln)\n","terms_bce = -(y_bce * np.log(yhat_bce) + (1 - y_bce) * np.log(1 - yhat_bce))\n","bce = terms_bce.mean()\n","\n","print(\"=== BCE (Entropía Cruzada Binaria) ===\")\n","print(\"i | y_i | ŷ_i  | -[y_i ln(ŷ_i) + (1 - y_i) ln(1 - ŷ_i)]\")\n","for i, (yi, yhi, ti) in enumerate(zip(y_bce, yhat_bce, terms_bce), start=1):\n","    print(f\"{i:1d} | {int(yi)}   | {yhi:6.3f} | {ti:10.6f}\")\n","print(\"BCE promedio =\", bce)\n","print()\n","\n","# ------------------------------------------------\n","# CE multiclase: Categorical Cross-Entropy\n","# ------------------------------------------------\n","y_true_onehot = np.array([\n","    [1, 0, 0],\n","    [0, 1, 0],\n","    [0, 0, 1],\n","    [1, 0, 0],\n","    [0, 0, 1]\n","], dtype=float)\n","\n","y_pred_probs = np.array([\n","    [0.70, 0.20, 0.10],\n","    [0.10, 0.60, 0.30],\n","    [0.20, 0.10, 0.70],\n","    [0.85, 0.10, 0.05],\n","    [0.05, 0.25, 0.70]\n","], dtype=float)\n","\n","y_pred_probs = np.clip(y_pred_probs, eps, 1 - eps)\n","\n","correct_idx = y_true_onehot.argmax(axis=1)\n","correct_probs = y_pred_probs[np.arange(len(correct_idx)), correct_idx]\n","\n","# Logaritmo natural ln()\n","ce_terms = -np.log(correct_probs)\n","ce = ce_terms.mean()\n","\n","print(\"=== CE (Entropía Cruzada Multiclase) ===\")\n","print(\"i | clase | prob_correcta | -ln(prob_correcta)\")\n","for i, (cls, p, t) in enumerate(zip(correct_idx + 1, correct_probs, ce_terms), start=1):\n","    print(f\"{i:1d} | {cls:6d} | {p:14.3f} | {t:14.9f}\")\n","print(\"CE promedio =\", ce)\n","print()\n","\n"]},{"cell_type":"markdown","source":["# Función Softmax\n","\n","Al final de la explicación de la \"Categorical Cross-Entropy\" se mencionó a la función de activación **softmax**. Ahora profundizaremos en ella.\n","\n","## Definición\n","La **función Softmax** convierte un conjunto de valores reales (llamados *logits*) en **probabilidades normalizadas**, que suman 1.  \n","Se usa típicamente en la **última capa** de modelos de clasificación multiclase.\n","\n","Dada una muestra con $C$ clases y logits $z_1, z_2, ..., z_C$, la función softmax se define como:\n","\n","$$\n","\\hat{y}_j = \\frac{e^{z_j}}{\\sum_{k=1}^{C} e^{z_k}} \\quad \\text{para } j = 1, 2, ..., C\n","$$\n","\n","donde:\n","- $z_j$: valor de activación (logit) antes del softmax para la clase $j$.  \n","- $\\hat{y}_j$: probabilidad predicha para la clase $j$.  \n","- $e^{z_j}$: transforma los logits en valores positivos y realza las diferencias relativas entre ellos.  \n","\n","El resultado $\\hat{y}$ es un vector de probabilidades tal que:\n","$$\n","\\sum_{j=1}^{C}\\hat{y}_j = 1\n","$$\n","\n","---\n","\n","La función Softmax **amplifica las diferencias relativas** entre los logits:\n","- Un valor de logit mucho mayor que los demás produce una probabilidad cercana a 1.  \n","- Logits similares generan probabilidades similares.  \n","- Nunca produce ceros ni unos exactos (a diferencia del argmax).  \n","\n","En resumen:\n","- Actúa como una versión “suavizada” del **argmax**.  \n","- Permite que el modelo exprese incertidumbre: por ejemplo, `[0.6, 0.3, 0.1]` en lugar de una decisión rígida.\n","\n","---\n","\n","## Ejemplo numérico\n","\n","Supongamos un modelo con tres clases y logits:\n","\n","$$\n","z = [2.0,\\; 1.0,\\; 0.1]\n","$$\n","\n","1. Calculamos los exponenciales:\n","$$\n","e^{z} = [e^{2.0},\\; e^{1.0},\\; e^{0.1}] = [7.389,\\; 2.718,\\; 1.105]\n","$$\n","\n","2. Sumamos los valores:\n","$$\n","\\sum e^{z_k} = 7.389 + 2.718 + 1.105 = 11.212\n","$$\n","\n","3. Normalizamos:\n","$$\n","\\hat{y}_1 = \\frac{7.389}{11.212} = 0.659\n","$$\n","$$\n","\\hat{y}_2 = \\frac{2.718}{11.212} = 0.243\n","$$\n","$$\n","\\hat{y}_3 = \\frac{1.105}{11.212} = 0.098\n","$$\n","\n","Resultado final:\n","| Clase | Logit ($z_j$) | $e^{z_j}$ | Probabilidad ($\\hat{y}_j$) |\n","|--------|---------------|-----------|-----------------------------|\n","| 1 | 2.0 | 7.389 | 0.659 |\n","| 2 | 1.0 | 2.718 | 0.243 |\n","| 3 | 0.1 | 1.105 | 0.098 |\n","\n","La suma de las probabilidades es:\n","$$\n","0.659 + 0.243 + 0.098 = 1.000\n","$$\n","\n","---\n","\n","## Propiedades importantes\n","\n","1. **Normalización:**  \n","   La salida siempre suma 1, por lo que puede interpretarse como una distribución de probabilidad.\n","\n","2. **Sensibilidad relativa:**  \n","   Multiplicar todos los logits por un número mayor hace las diferencias más marcadas (las probabilidades tienden más hacia 0 o 1).\n","\n","3. **Invarianza a traslaciones:**  \n","   Si sumamos una constante a todos los logits, la salida del softmax no cambia:\n","   $$\n","   \\text{softmax}(z) = \\text{softmax}(z + c)\n","   $$\n","   Esto se usa para **estabilizar numéricamente** la función restando el logit máximo antes de aplicar $e^{z}$.\n","\n","---\n","\n","## Implementación estable numéricamente\n","\n","Para evitar desbordamientos al calcular $e^{z_j}$ (si los logits son grandes), se usa la versión estable:\n","\n","$$\n","\\hat{y}_j = \\frac{e^{z_j - \\max(z)}}{\\sum_{k=1}^{C} e^{z_k - \\max(z)}}\n","$$\n","\n","Esto no cambia el resultado matemático, pero evita problemas de precisión.\n","\n","---\n","\n","## En redes neuronales\n","- Se usa en la **última capa** de modelos de clasificación multiclase.  \n","- Convierte los logits en probabilidades que pueden compararse con las etiquetas one hot usando la **Categorical Cross-Entropy (CE)**:\n","  $$\n","  \\text{CE} = -\\sum_{c=1}^{C} y_c \\ln(\\hat{y}_c)\n","  $$\n","\n","Softmax y Cross-Entropy se implementan juntas en la mayoría de librerías (por ejemplo, `nn.CrossEntropyLoss` en PyTorch) porque su derivada combinada es más estable y eficiente.\n","\n","---\n","\n","## Finalmente\n","\n","Softmax no decide directamente la clase, sino que **asigna confianza**.  \n","El modelo luego elige la clase con mayor probabilidad:\n","$$\n","\\text{Clase predicha} = \\arg\\max_j(\\hat{y}_j)\n","$$\n","\n","Así, Softmax convierte las salidas crudas del modelo en una distribución interpretable, facilitando el cálculo de la pérdida y la interpretación de las predicciones.\n"],"metadata":{"id":"sh58JufHBr_7"}},{"cell_type":"markdown","source":["---\n","## ¿Por qué la función Softmax amplifica las diferencias relativas entre los logits?\n","\n","La clave del comportamiento de **Softmax** está en la **exponencial ($e^{z_j}$)** presente en su definición:\n","\n","$$\n","\\hat{y}_j = \\frac{e^{z_j}}{\\sum_{k=1}^{C} e^{z_k}}\n","$$\n","\n","La función exponencial **crece muy rápido** para valores grandes y **disminuye rápidamente** para valores pequeños.  \n","Esto provoca que **pequeñas diferencias en los logits se transformen en diferencias mucho mayores en las probabilidades**.\n","\n","---\n","\n","### 1. Ejemplo simple\n","\n","Supongamos dos clases con logits:\n","\n","| Clase | Logit $z_j$ | $e^{z_j}$ | Probabilidad $\\hat{y}_j$ |\n","|--------|--------------|------------|---------------------------|\n","| A | 2.0 | 7.389 | 0.731 |\n","| B | 1.0 | 2.718 | 0.269 |\n","\n","Aunque la diferencia entre los logits es solo **1 unidad**,  \n","la diferencia entre las probabilidades finales es de **0.731 - 0.269 = 0.462**.  \n","Softmax amplifica ese contraste.\n","\n","---\n","\n","### 2. Si aumentamos ligeramente la diferencia de logits\n","\n","| Clase | Logit $z_j$ | $e^{z_j}$ | Probabilidad $\\hat{y}_j$ |\n","|--------|--------------|------------|---------------------------|\n","| A | 2.5 | 12.182 | 0.818 |\n","| B | 1.0 | 2.718 | 0.182 |\n","\n","Ahora la diferencia de logits es 1.5,  \n","pero la diferencia de probabilidades sube a **0.818 - 0.182 = 0.636**.  \n","\n","**El crecimiento exponencial convierte incrementos lineales en aumentos no lineales de confianza.**\n","\n","---\n","\n","### 3. Consecuencia práctica\n","\n","Esto significa que:\n","- Softmax **refuerza las clases más probables** (las hace aún más probables).  \n","- Reduce aún más las probabilidades de las clases menos favorecidas.  \n","- Hace que el modelo sea más **decisivo**, pero también más **sensible** a los cambios en los logits.\n","\n","Por eso, una pequeña mejora en el logit de la clase correcta puede traducirse en un gran aumento en su probabilidad final.  \n","Del mismo modo, un pequeño error en los logits puede hacer que el modelo se vuelva demasiado confiado en una clase incorrecta.\n","\n","---\n","\n","### 4. Comparación visual\n","\n","Si graficamos la función exponencial y una función lineal para el mismo rango de logits:\n","\n","| $z$ | $e^z$ | $z$ |\n","|-----|-------|-----|\n","| -1 | 0.367 | -1 |\n","| 0 | 1 | 0 |\n","| 1 | 2.718 | 1 |\n","| 2 | 7.389 | 2 |\n","| 3 | 20.085 | 3 |\n","\n","Mientras una función lineal aumenta de manera constante, la exponencial crece **de forma explosiva**.  \n","Softmax hereda esa propiedad: **amplifica la diferencia entre logits grandes y pequeños** antes de normalizarlos.\n","\n","---\n","\n","### 5. Implicación en el aprendizaje\n","\n","- Si el modelo produce logits muy cercanos entre sí, las probabilidades estarán **repartidas** (el modelo es incierto).  \n","- Si una clase obtiene un logit un poco mayor, Softmax la hace **dominante**, empujando su probabilidad cerca de 1.  \n","- En el entrenamiento, esto incentiva a las redes a **separar más claramente las clases** para minimizar la entropía cruzada.\n","\n","---\n","\n","### 6. Conclusiones\n","\n","Softmax actúa como un **amplificador de confianza**:\n","- Convierte diferencias pequeñas en decisiones más claras.  \n","- Potencia la clase más probable, sin eliminar completamente las demás.  \n","- Permite que la red exprese **“cuán seguro está”** de su predicción.\n","\n","Por eso, en la práctica, los logits son proporcionales a la **confianza bruta del modelo**,  \n","y Softmax los traduce en una distribución de probabilidad interpretable, **amplificando las diferencias relativas** entre ellos.\n"],"metadata":{"id":"PRgZWTZOCftI"}}]}