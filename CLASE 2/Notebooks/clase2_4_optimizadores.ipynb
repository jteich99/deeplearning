{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNhWLDf+zf4FxvzRVjeCtoo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Optimizadores en Aprendizaje Automático\n","\n","A continuación se explican los principales algoritmos de optimización usados para entrenar redes neuronales.\n","\n","Como ejemplo rápido se usarán \"redes\" de una sola neurona (sin capas ocultas) para un caso simple de regresión lineal.\n","\n","---\n","\n","## 1. Gradient Descent (GD) – Descenso por Gradiente\n","\n","### Idea general\n","Calcula el **gradiente completo** del costo usando **todo el dataset** y actualiza los parámetros una sola vez por iteración (época).\n","\n","El gradiente indica la dirección de **mayor incremento** de la función de costo $J(w, b)$.  \n","El algoritmo se mueve en la **dirección opuesta** para minimizarla:\n","\n","$$\n","\\Delta w = -\\eta \\nabla_w J(w, b)\n","$$\n","\n","donde $ \\eta $ es la tasa de aprendizaje o learning rate $(lr)$.\n","\n","### Función de costo\n","Usando el **Mean Squared Error (MSE)** como función de costo:\n","\n","$$\n","J(w,b) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat y_i)^2\n","$$\n","\n","donde $n$ es el número de muestras, $y_i$ es la etiqueta real y $\\hat y_i = w x_i + b$ la predicción.\n","\n","### Derivadas\n","Las derivadas parciales del costo respecto a los parámetros son:\n","\n","$$\n","\\frac{\\partial J}{\\partial w} = \\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\hat y_i)(-x_i)\n","$$\n","\n","$$\n","\\frac{\\partial J}{\\partial b} = \\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\hat y_i)(-1)\n","$$\n","\n","Al simplificar los signos:\n","\n","$$\n","\\frac{\\partial J}{\\partial w} = -\\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\hat y_i)x_i\n","$$\n","\n","$$\n","\\frac{\\partial J}{\\partial b} = -\\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\hat y_i)\n","$$\n","\n","### Actualización de parámetros\n","En cada paso del descenso:\n","\n","$$\n","w := w - \\eta \\frac{\\partial J}{\\partial w}\n","$$\n","\n","$$\n","b := b - \\eta \\frac{\\partial J}{\\partial b}\n","$$\n","\n","### Forma vectorizada\n","Para un modelo con varios parámetros, la actualización general es:\n","\n","$$\n","\\mathbf{w} := \\mathbf{w} - \\eta \\nabla_{\\mathbf{w}} J(\\mathbf{w})\n","$$\n","\n","donde $ \\nabla_{\\mathbf{w}} J $ es el vector gradiente de la función de costo respecto a todos los parámetros.\n","\n","### Ejemplo intuitivo\n","- Si $J(w,b)$ es una parábola convexa, el gradiente apunta hacia la pendiente ascendente.  \n","- El descenso por gradiente se mueve **en la dirección contraria** al gradiente, acercándose al mínimo global.\n","\n","### Consideraciones prácticas\n","- La cantidad de veces que se actualizan los parámetros es la misma que la cantidad de épocas de entrenamiento.\n","- Si $ \\eta $ es **muy grande**, el algoritmo puede **oscilar o divergir**.  \n","- Si $ \\eta $ es **muy pequeña**, la convergencia será **lenta**.   \n","- En funciones **no convexas**, puede quedar atrapado en **mínimos locales**.\n","\n","### Características\n","- Usa **todas** las muestras por paso.  \n","- Converge de manera **suave y estable**, pero puede ser **lento** en datasets grandes.\n","\n"],"metadata":{"id":"_NdgskDaG3d6"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":735,"output_embedded_package_id":"1NqNQqPBcoNt0Mj4jpgeVrizfGZYQ6mTW"},"id":"BmaLteXZ-ycu","executionInfo":{"status":"ok","timestamp":1761834199948,"user_tz":300,"elapsed":72621,"user":{"displayName":"Gerardo Alexis Vilcamiza Espinoza","userId":"15529282681696955571"}},"outputId":"f564b847-e9f3-4830-b730-52f92f0d3ef3"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","from IPython.display import HTML, display\n","\n","# --- Dataset sintético ---\n","rng = np.random.default_rng(7)\n","n = 200\n","X = rng.uniform(-3, 3, size=n)\n","w_true, b_true = 1.8, -0.7\n","noise = rng.normal(0, 0.6, size=n)\n","Y = w_true * X + b_true + noise\n","\n","# --- Función de costo MSE y su gradiente (derivada) ---\n","def J_mse(w, b):\n","    yhat = w * X + b\n","    e = Y - yhat\n","    return np.mean(e**2)\n","\n","def grad_mse(w, b):\n","    yhat = w * X + b\n","    e = Y - yhat\n","    n = len(X)\n","    dw = -(2/n) * np.sum(e * X)\n","    db = -(2/n) * np.sum(e)\n","    return dw, db\n","\n","# --- Parámetros para el entrenamiento ---\n","w0, b0 = -3.5, 3.5  # Valores iniciales de los parámetros (normalmente se eligen al azar)\n","lr = 0.02           # tasa de aprendizaje\n","epocas = 100        # Iteraciones o ciclos de entrenamiento (igual a la cantidad de actualizaciones de parámetros)\n","\n","# --- Algoritmo del descenso del gradiente (GD) ---\n","w, b = w0, b0\n","path_w, path_b, path_J = [w], [b], [J_mse(w, b)]\n","for _ in range(epocas - 1):\n","    dw, db = grad_mse(w, b)\n","    w = w - lr * dw     # Se actualiza el parámetro w (peso)\n","    b = b - lr * db     # Se actualiza el parámetro b (bias)\n","    path_w.append(w)\n","    path_b.append(b)\n","    path_J.append(J_mse(w, b))\n","\n","path_w, path_b, path_J = np.array(path_w), np.array(path_b), np.array(path_J)\n","\n","# --- Hasta acá acaba la lógica del algoritmo optimizador, el resto del código es más que nada para las graficas y animaciones ---\n","\n","\n","\n","# GRÁFICAS Y ANIMACIONES:\n","# --- Malla de la superficie del costo ---\n","pad_w, pad_b = 1.5, 1.5\n","wmin, wmax = min(path_w.min(), w_true) - pad_w, max(path_w.max(), w_true) + pad_w\n","bmin, bmax = min(path_b.min(), b_true) - pad_b, max(path_b.max(), b_true) + pad_b\n","\n","W = np.linspace(wmin, wmax, 140)\n","B = np.linspace(bmin, bmax, 140)\n","WW, BB = np.meshgrid(W, B)\n","JJ = np.zeros_like(WW)\n","for i in range(WW.shape[0]):\n","    yhat_grid = WW[i][None, :] * X[:, None] + BB[i][None, :]\n","    JJ[i] = np.mean((Y[:, None] - yhat_grid) ** 2, axis=0)\n","\n","# --- Figura y ejes ---\n","fig = plt.figure(figsize=(12, 6))\n","ax3d = fig.add_subplot(1, 2, 1, projection=\"3d\")\n","ax2d = fig.add_subplot(1, 2, 2)\n","\n","# Superficie 3D\n","surf = ax3d.plot_surface(WW, BB, JJ, alpha=0.82, linewidth=0, antialiased=True)\n","ax3d.set_xlabel(\"w\")\n","ax3d.set_ylabel(\"b\")\n","ax3d.set_zlabel(\"J(w,b)\")\n","ax3d.view_init(elev=30, azim=45)\n","ax3d.set_xlim(wmin, wmax)\n","ax3d.set_ylim(bmin, bmax)\n","ax3d.set_zlim(0, JJ.max() * 1.05)\n","\n","# Trayectorias\n","trail3d, = ax3d.plot([], [], [], lw=2, color=\"k\")\n","point3d = ax3d.scatter([], [], [], s=60, c=\"red\")\n","ax3d.scatter([path_w[0]], [path_b[0]], [path_J[0]], s=60, c=\"orange\", marker=\"x\")\n","\n","# Contornos 2D (vista superior)\n","cs = ax2d.contourf(WW, BB, JJ, levels=30)\n","ax2d.contour(WW, BB, JJ, levels=15, colors=\"k\", linewidths=0.4, alpha=0.6)\n","ax2d.set_xlabel(\"w\")\n","ax2d.set_ylabel(\"b\")\n","ax2d.set_xlim(wmin, wmax)\n","ax2d.set_ylim(bmin, bmax)\n","trail2d, = ax2d.plot([], [], lw=2, color=\"k\")\n","point2d, = ax2d.plot([], [], marker=\"o\", markersize=6, color=\"red\")\n","\n","# --- Texto dinámico ---\n","info_text = fig.text(\n","    0.12, 0.93,\n","    f\"J(w,b) = {path_J[0]:.6f}   |   w = {path_w[0]:.4f}, b = {path_b[0]:.4f}   |   lr = {lr}\",\n","    fontsize=12, weight=\"bold\"\n",")\n","grad_text = fig.text(0.12, 0.90, \"\", fontsize=10)\n","\n","# --- Funciones de animación ---\n","def init():\n","    trail3d.set_data([], [])\n","    trail3d.set_3d_properties([])\n","    point3d._offsets3d = ([], [], [])\n","    trail2d.set_data([], [])\n","    point2d.set_data([], [])\n","    info_text.set_text(f\"J(w,b) = {path_J[0]:.6f}   |   w = {path_w[0]:.4f}, b = {path_b[0]:.4f}   |   lr = {lr}\")\n","    grad_text.set_text(\"\")\n","    return trail3d, point3d, trail2d, point2d, info_text, grad_text\n","\n","def update(i):\n","    trail3d.set_data(path_w[:i+1], path_b[:i+1])\n","    trail3d.set_3d_properties(path_J[:i+1])\n","    point3d._offsets3d = (np.array([path_w[i]]), np.array([path_b[i]]), np.array([path_J[i]]))\n","    trail2d.set_data(path_w[:i+1], path_b[:i+1])\n","    point2d.set_data([path_w[i]], [path_b[i]])\n","\n","    dw, db = grad_mse(path_w[i], path_b[i])\n","    info_text.set_text(\n","        f\"J(w,b) = {path_J[i]:.6f}   |   w = {path_w[i]:.4f}, b = {path_b[i]:.4f}   |   lr = {lr}\"\n","    )\n","    grad_text.set_text(f\"dJ/dw = {dw:.4f}   dJ/db = {db:.4f}\")\n","    return trail3d, point3d, trail2d, point2d, info_text, grad_text\n","\n","ani = animation.FuncAnimation(fig, update, frames=len(path_w), init_func=init, interval=80, blit=False)\n","\n","# --- Mostrar como HTML ---\n","html = ani.to_jshtml()\n","plt.close(fig)\n","display(HTML(html))\n"]},{"cell_type":"markdown","source":["## 2. Stochastic Gradient Descent (SGD) – Descenso Estocástico por Gradiente\n","\n","### Idea general\n","En lugar de usar **todo el dataset** como en el GD clásico, el SGD actualiza los parámetros **tras cada muestra individual**.  \n","Esto introduce cierta aleatoriedad (estocasticidad) en la trayectoria, lo que puede ayudar a escapar de mínimos locales.\n","\n","Cada paso usa una sola muestra $(x_i, y_i)$ para aproximar el gradiente verdadero.  \n","El algoritmo se mueve en la dirección contraria al gradiente estimado:\n","\n","$$\n","\\Delta w = -\\eta \\nabla_w J_i(w,b)\n","$$\n","\n","donde $J_i(w,b)$ es el costo calculado solo con la muestra $i$ y $ \\eta $ es la tasa de aprendizaje o learning rate $(lr)$.\n","\n","### Función de costo\n","El costo individual de una muestra se define como:\n","\n","$$\n","J_i(w,b) = (y_i - \\hat y_i)^2\n","$$\n","\n","donde $\\hat y_i = w x_i + b$ es la predicción del modelo lineal.\n","\n","### Derivadas\n","Para esa única muestra se tienen las siguientes gradientes individuales:\n","\n","$$\n","\\frac{\\partial J_i}{\\partial w} = -2 (y_i - \\hat y_i) x_i\n","$$\n","\n","$$\n","\\frac{\\partial J_i}{\\partial b} = -2 (y_i - \\hat y_i)\n","$$\n","\n","### Actualización de parámetros\n","En cada iteración (una muestra), se actualiza inmediatamente:\n","\n","$$\n","w := w - \\eta \\frac{\\partial J_i}{\\partial w}\n","$$\n","\n","$$\n","b := b - \\eta \\frac{\\partial J_i}{\\partial b}\n","$$\n","\n","y luego se pasa a la siguiente muestra (posiblemente en orden aleatorio).\n","\n","### Forma vectorizada\n","Para datasets grandes, se expresa de forma equivalente:\n","\n","$$\n","\\mathbf{w} := \\mathbf{w} - \\eta \\nabla_{\\mathbf{w}} J_i(\\mathbf{w})\n","$$\n","\n","donde el gradiente se calcula con **una sola muestra** o un subconjunto aleatorio pequeño.\n","\n","### Ejemplo intuitivo\n","- El SGD se mueve en zigzag, porque el gradiente de cada muestra apunta en una dirección ligeramente diferente.  \n","- Aunque parece inestable, ese ruido puede ayudar a salir de valles planos o mínimos locales donde el GD clásico se quedaría encerrado.  \n","- Con suficiente número de iteraciones, converge al mismo punto que el GD, pero con una trayectoria más ruidosa.\n","\n","### Consideraciones prácticas\n","- La cantidad de veces que se actualizan los parámetros es la cantidad de épocas de entrenamiento por la cantidad de muestras del dataset. Por lo que es un proceso más largo y lento que el GD clásico.\n","- Requiere **barajar aleatoriamente** las muestras en cada época para evitar sesgos.  \n","- Su convergencia es **más rápida por iteración**, pero **más ruidosa**.  \n","- Puede oscilar cerca del mínimo, por lo que a veces se aplica **momentum** o una **tasa de aprendizaje decreciente**.  \n","- Ideal para datasets **muy grandes**, donde calcular el gradiente completo sería costoso.\n","\n","### Características\n","- Actualiza los parámetros **una vez por muestra**.  \n","- Introduce **ruido beneficioso** para explorar el espacio de soluciones.  \n","- Puede **no converger exactamente**, pero sí acercarse al mínimo con fluctuaciones pequeñas.\n"],"metadata":{"id":"KdknKdUhPnXN"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","from IPython.display import HTML, display\n","\n","# --- Dataset sintético ---\n","rng = np.random.default_rng(7)\n","n = 20\n","X = rng.uniform(-3, 3, size=n)\n","w_true, b_true = 1.8, -0.7\n","noise = rng.normal(0, 0.6, size=n)\n","Y = w_true * X + b_true + noise\n","\n","# --- Función de costo MSE y sus gradientes individuales ---\n","def J_mse(w, b):\n","    yhat = w * X + b\n","    e = Y - yhat\n","    return np.mean(e**2)\n","\n","def grad_sample(w, b, x_i, y_i):\n","    yhat_i = w * x_i + b\n","    e_i = y_i - yhat_i\n","    dw = -2 * e_i * x_i\n","    db = -2 * e_i\n","    return dw, db\n","\n","# --- Parámetros para el entrenamiento ---\n","w0, b0 = -3.5, 3.5   # Valores iniciales de los parámetros (normalmente se eligen al azar)\n","lr = 0.02            # tasa de aprendizaje\n","epocas = 20          # cada época recorre todas las muestras\n","steps = epocas * n   # la cantidad de actualizaciones ya no es igual a las epocas como en GD, ahora se multiplica por la cantidad de muestras\n","\n","# --- Algoritmo del descenso estocástico del gradiente (SGD) ---\n","w, b = w0, b0\n","path_w, path_b, path_J = [w], [b], [J_mse(w, b)]\n","\n","for epoch in range(epocas):\n","    indices = rng.permutation(n)\n","    for i in indices:\n","        dw, db = grad_sample(w, b, X[i], Y[i])\n","        w = w - lr * dw     # Se actualiza el parámetro w (peso)\n","        b = b - lr * db     # Se actualiza el parámetro b (bias)\n","        path_w.append(w)\n","        path_b.append(b)\n","        path_J.append(J_mse(w, b))\n","\n","path_w, path_b, path_J = np.array(path_w), np.array(path_b), np.array(path_J)\n","\n","# --- Hasta acá acaba la lógica del algoritmo optimizador, el resto del código es más que nada para las graficas y animaciones ---\n","\n","\n","\n","# GRÁFICAS Y ANIMACIONES:\n","# --- Malla de la superficie del costo ---\n","pad_w, pad_b = 1.5, 1.5\n","wmin, wmax = min(path_w.min(), w_true) - pad_w, max(path_w.max(), w_true) + pad_w\n","bmin, bmax = min(path_b.min(), b_true) - pad_b, max(path_b.max(), b_true) + pad_b\n","\n","W = np.linspace(wmin, wmax, 140)\n","B = np.linspace(bmin, bmax, 140)\n","WW, BB = np.meshgrid(W, B)\n","JJ = np.zeros_like(WW)\n","for i in range(WW.shape[0]):\n","    yhat_grid = WW[i][None, :] * X[:, None] + BB[i][None, :]\n","    JJ[i] = np.mean((Y[:, None] - yhat_grid) ** 2, axis=0)\n","\n","# --- Figura y ejes ---\n","fig = plt.figure(figsize=(12, 6))\n","ax3d = fig.add_subplot(1, 2, 1, projection=\"3d\")\n","ax2d = fig.add_subplot(1, 2, 2)\n","\n","surf = ax3d.plot_surface(WW, BB, JJ, alpha=0.82, linewidth=0, antialiased=True)\n","ax3d.set_xlabel(\"w\")\n","ax3d.set_ylabel(\"b\")\n","ax3d.set_zlabel(\"J(w,b)\")\n","ax3d.view_init(elev=30, azim=45)\n","ax3d.set_xlim(wmin, wmax)\n","ax3d.set_ylim(bmin, bmax)\n","ax3d.set_zlim(0, JJ.max() * 1.05)\n","\n","trail3d, = ax3d.plot([], [], [], lw=2, color=\"k\")\n","point3d = ax3d.scatter([], [], [], s=60, c=\"red\")\n","ax3d.scatter([path_w[0]], [path_b[0]], [path_J[0]], s=60, c=\"orange\", marker=\"x\")\n","\n","# Contornos 2D\n","cs = ax2d.contourf(WW, BB, JJ, levels=30)\n","ax2d.contour(WW, BB, JJ, levels=15, colors=\"k\", linewidths=0.4, alpha=0.6)\n","ax2d.set_xlabel(\"w\")\n","ax2d.set_ylabel(\"b\")\n","ax2d.set_xlim(wmin, wmax)\n","ax2d.set_ylim(bmin, bmax)\n","trail2d, = ax2d.plot([], [], lw=2, color=\"k\")\n","point2d, = ax2d.plot([], [], marker=\"o\", markersize=6, color=\"red\")\n","\n","# --- Texto dinámico ---\n","info_text = fig.text(\n","    0.12, 0.93,\n","    f\"J(w,b) = {path_J[0]:.6f}   |   w = {path_w[0]:.4f}, b = {path_b[0]:.4f}   |   lr = {lr}\",\n","    fontsize=12, weight=\"bold\"\n",")\n","grad_text = fig.text(0.12, 0.90, \"SGD - 1 muestra por iteración\", fontsize=10)\n","\n","# --- Funciones de animación ---\n","def init():\n","    trail3d.set_data([], [])\n","    trail3d.set_3d_properties([])\n","    point3d._offsets3d = ([], [], [])\n","    trail2d.set_data([], [])\n","    point2d.set_data([], [])\n","    info_text.set_text(f\"J(w,b) = {path_J[0]:.6f}   |   w = {path_w[0]:.4f}, b = {path_b[0]:.4f}   |   lr = {lr}\")\n","    grad_text.set_text(\"1 muestra por iteración\")\n","    return trail3d, point3d, trail2d, point2d, info_text, grad_text\n","\n","def update(i):\n","    trail3d.set_data(path_w[:i+1], path_b[:i+1])\n","    trail3d.set_3d_properties(path_J[:i+1])\n","    point3d._offsets3d = (np.array([path_w[i]]), np.array([path_b[i]]), np.array([path_J[i]]))\n","    trail2d.set_data(path_w[:i+1], path_b[:i+1])\n","    point2d.set_data([path_w[i]], [path_b[i]])\n","    info_text.set_text(\n","        f\"J(w,b) = {path_J[i]:.6f}   |   w = {path_w[i]:.4f}, b = {path_b[i]:.4f}   |   lr = {lr}\"\n","    )\n","    return trail3d, point3d, trail2d, point2d, info_text, grad_text\n","\n","ani = animation.FuncAnimation(fig, update, frames=len(path_w), init_func=init, interval=60, blit=False)\n","\n","# --- Mostrar como HTML ---\n","html = ani.to_jshtml()\n","plt.close(fig)\n","display(HTML(html))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":735,"output_embedded_package_id":"1B8fylk4zgSxCTkp1Xbwk3DF2pvANfNw1"},"id":"f4_6Y1k4AtcM","executionInfo":{"status":"ok","timestamp":1761834345535,"user_tz":300,"elapsed":145403,"user":{"displayName":"Gerardo Alexis Vilcamiza Espinoza","userId":"15529282681696955571"}},"outputId":"aacb3e08-10e3-43b8-b5fd-befe2a48b617"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["## 3. Adam Full-batch (Adaptive Moment Estimation) – Descenso Adaptativo por Gradiente\n","\n","### Idea general\n","**Adam** es un optimizador que adapta automáticamente la magnitud de la actualización de cada parámetro, basándose en el historial de sus gradientes.  \n","A diferencia del GD o el SGD, que aplican la misma tasa de aprendizaje a todos los parámetros, Adam ajusta la velocidad de actualización de forma independiente para cada uno.\n","\n","Adam mantiene un **promedio del gradiente reciente** y también un **promedio del cuadrado de los gradientes**.  \n","Esto le permite saber:\n","- En qué dirección se ha estado moviendo el gradiente (su tendencia).\n","- Cuán grandes o variables han sido esos gradientes (su escala).\n","\n","Con esa información, Adam acelera el descenso en direcciones estables y lo frena en direcciones ruidosas.\n","\n","### Función de costo\n","Como en los casos anteriores, el costo usado es el **Error Cuadrático Medio (MSE)**:\n","\n","$$\n","J(w,b) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat y_i)^2\n","$$\n","\n","donde $\\hat y_i = w x_i + b$.\n","\n","### Derivadas\n","Las derivadas del costo son las mismas que en GD:\n","\n","$$\n","\\frac{\\partial J}{\\partial w} = -\\frac{2}{n}\\sum_{i=1}^{n}(y_i - \\hat y_i)x_i\n","$$\n","\n","$$\n","\\frac{\\partial J}{\\partial b} = -\\frac{2}{n}\\sum_{i=1}^{n}(y_i - \\hat y_i)\n","$$\n","\n","### Actualización de Adam\n","En cada paso de entrenamiento (iteración $t$), Adam realiza tres etapas:\n","\n","1. **Acumula promedios móviles de los gradientes:**\n","\n","   Guarda una “memoria” del gradiente promedio $m_t$ y del cuadrado del gradiente promedio $v_t$:\n","\n","   $$\n","   m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t\n","   $$\n","   $$\n","   v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\n","   $$\n","\n","   donde $g_t$ es el gradiente actual y $\\beta_1, \\beta_2$ son factores de suavizado (por defecto, $\\beta_1 = 0.9$, $\\beta_2 = 0.999$).\n","\n","2. **Corrige el sesgo de los promedios:**\n","\n","   Al inicio, los promedios están sesgados hacia cero. Se corrigen dividiéndolos por su factor de decaimiento:\n","\n","   $$\n","   \\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t},\n","   $$\n","\n","   $$\n","   \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n","   $$\n","\n","3. **Actualiza los parámetros:**\n","\n","   Los gradientes se escalan de forma adaptativa según la magnitud de $\\hat{v}_t$:\n","\n","   $$\n","   w_t = w_{t-1} - \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon},\n","   $$\n","\n","   $$\n","   b_t = b_{t-1} - \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n","   $$\n","\n","   donde $\\eta$ es la tasa de aprendizaje $(lr)$ y $\\epsilon$ es un valor pequeño (por ejemplo $10^{-8}$) para evitar divisiones por cero.\n","\n","### Forma vectorizada\n","Para modelos con varios parámetros:\n","\n","$$\n","\\mathbf{w}_t = \\mathbf{w}_{t-1} - \\eta \\frac{\\hat{\\mathbf{m}}_t}{\\sqrt{\\hat{\\mathbf{v}}_t} + \\epsilon}\n","$$\n","\n","### Ejemplo intuitivo\n","- Si los gradientes son pequeños pero consistentes, Adam los **refuerza**, avanzando más rápido.  \n","- Si los gradientes cambian mucho de signo o magnitud, Adam los **suaviza**, reduciendo el ruido.  \n","- Así, aprende más rápido que GD y con mayor estabilidad que SGD.\n","\n","### Consideraciones prácticas\n","- Usa **valores por defecto muy estables**: $\\beta_1 = 0.9$, $\\beta_2 = 0.999$, $\\epsilon = 10^{-8}$.  \n","- Puede usar una tasa de aprendizaje mayor que GD sin volverse inestable.  \n","- Generalmente converge más rápido y de forma más suave.  \n","- Es uno de los optimizadores más usados en la práctica por su rendimiento robusto.\n","\n","### Características\n","- Calcula el gradiente usando **todo el dataset (full batch)**.  \n","- Ajusta el paso de aprendizaje de **forma adaptativa** por parámetro.  \n","- Converge **rápido y de manera estable**, incluso en funciones no convexas.  \n","- Reduce la sensibilidad al valor de $\\eta$ y acelera el aprendizaje temprano.\n"],"metadata":{"id":"v2gjE3egXkoo"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","from IPython.display import HTML, display\n","\n","# --- Dataset sintético ---\n","rng = np.random.default_rng(7)\n","n = 200\n","X = rng.uniform(-3, 3, size=n)\n","w_true, b_true = 1.8, -0.7\n","noise = rng.normal(0, 0.6, size=n)\n","Y = w_true * X + b_true + noise\n","\n","# --- Función de costo MSE y su gradiente (derivada) ---\n","def J_mse(w, b):\n","    yhat = w * X + b\n","    e = Y - yhat\n","    return np.mean(e**2)\n","\n","def grad_full(w, b):\n","    yhat = w * X + b\n","    e = Y - yhat\n","    n = len(X)\n","    dw = -(2/n) * np.sum(e * X)\n","    db = -(2/n) * np.sum(e)\n","    return dw, db\n","\n","# --- Parámetros para el entrenamiento ---\n","w0, b0 = -3.5, 3.5  # Valores iniciales de los parámetros (normalmente se eligen al azar)\n","lr = 0.05           # tasa de aprendizaje\n","beta1, beta2 = 0.9, 0.999   # se utilizan los betas por defecto, ya que suelen dar buenos resultados\n","eps = 1e-8          # valor muy pequeño para epsilon\n","epocas = 150        # Iteraciones o ciclos de entrenamiento (igual a la cantidad de actualizaciones de parámetros)\n","\n","# --- Algoritmo Adam Full-batch ---\n","w, b = w0, b0\n","m_w = m_b = 0.0\n","v_w = v_b = 0.0\n","\n","path_w = [w]\n","path_b = [b]\n","path_J = [J_mse(w, b)]\n","\n","for t in range(1, epocas):\n","    dw, db = grad_full(w, b)\n","    m_w = beta1 * m_w + (1 - beta1) * dw\n","    m_b = beta1 * m_b + (1 - beta1) * db\n","    v_w = beta2 * v_w + (1 - beta2) * (dw * dw)\n","    v_b = beta2 * v_b + (1 - beta2) * (db * db)\n","    m_w_hat = m_w / (1 - beta1**t)\n","    m_b_hat = m_b / (1 - beta1**t)\n","    v_w_hat = v_w / (1 - beta2**t)\n","    v_b_hat = v_b / (1 - beta2**t)\n","    w = w - lr * m_w_hat / (np.sqrt(v_w_hat) + eps)\n","    b = b - lr * m_b_hat / (np.sqrt(v_b_hat) + eps)\n","    path_w.append(w)\n","    path_b.append(b)\n","    path_J.append(J_mse(w, b))\n","\n","path_w = np.array(path_w)\n","path_b = np.array(path_b)\n","path_J = np.array(path_J)\n","\n","# --- Hasta acá acaba la lógica del algoritmo optimizador, el resto del código es más que nada para las graficas y animaciones ---\n","\n","\n","\n","# GRÁFICAS Y ANIMACIONES:\n","# --- Malla de la superficie del costo ---\n","pad_w, pad_b = 1.5, 1.5\n","wmin, wmax = min(path_w.min(), w_true) - pad_w, max(path_w.max(), w_true) + pad_w\n","bmin, bmax = min(path_b.min(), b_true) - pad_b, max(path_b.max(), b_true) + pad_b\n","\n","W = np.linspace(wmin, wmax, 140)\n","B = np.linspace(bmin, bmax, 140)\n","WW, BB = np.meshgrid(W, B)\n","JJ = np.zeros_like(WW)\n","for i in range(WW.shape[0]):\n","    yhat_grid = WW[i][None, :] * X[:, None] + BB[i][None, :]\n","    JJ[i] = np.mean((Y[:, None] - yhat_grid) ** 2, axis=0)\n","\n","fig = plt.figure(figsize=(12, 6))\n","ax3d = fig.add_subplot(1, 2, 1, projection=\"3d\")\n","ax2d = fig.add_subplot(1, 2, 2)\n","\n","surf = ax3d.plot_surface(WW, BB, JJ, alpha=0.82, linewidth=0, antialiased=True)\n","ax3d.set_xlabel(\"w\")\n","ax3d.set_ylabel(\"b\")\n","ax3d.set_zlabel(\"J(w,b)\")\n","ax3d.view_init(elev=30, azim=45)\n","ax3d.set_xlim(wmin, wmax)\n","ax3d.set_ylim(bmin, bmax)\n","ax3d.set_zlim(0, JJ.max() * 1.05)\n","\n","trail3d, = ax3d.plot([], [], [], lw=2, color=\"k\")\n","point3d = ax3d.scatter([], [], [], s=60, c=\"red\")\n","ax3d.scatter([path_w[0]], [path_b[0]], [path_J[0]], s=60, c=\"orange\", marker=\"x\")\n","\n","cs = ax2d.contourf(WW, BB, JJ, levels=30)\n","ax2d.contour(WW, BB, JJ, levels=15, colors=\"k\", linewidths=0.4, alpha=0.6)\n","ax2d.set_xlabel(\"w\")\n","ax2d.set_ylabel(\"b\")\n","ax2d.set_xlim(wmin, wmax)\n","ax2d.set_ylim(bmin, bmax)\n","trail2d, = ax2d.plot([], [], lw=2, color=\"k\")\n","point2d, = ax2d.plot([], [], marker=\"o\", markersize=6, color=\"red\")\n","\n","info_text = fig.text(\n","    0.12, 0.93,\n","    f\"J(w,b) = {path_J[0]:.6f}   |   w = {path_w[0]:.4f}, b = {path_b[0]:.4f}   |   lr = {lr}\",\n","    fontsize=12, weight=\"bold\"\n",")\n","grad_text = fig.text(0.12, 0.90, \"\", fontsize=10)\n","\n","def init():\n","    trail3d.set_data([], [])\n","    trail3d.set_3d_properties([])\n","    point3d._offsets3d = ([], [], [])\n","    trail2d.set_data([], [])\n","    point2d.set_data([], [])\n","    info_text.set_text(f\"J(w,b) = {path_J[0]:.6f}   |   w = {path_w[0]:.4f}, b = {path_b[0]:.4f}   |   lr = {lr}\")\n","    grad_text.set_text(\"\")\n","    return trail3d, point3d, trail2d, point2d, info_text, grad_text\n","\n","def update(i):\n","    trail3d.set_data(path_w[:i+1], path_b[:i+1])\n","    trail3d.set_3d_properties(path_J[:i+1])\n","    point3d._offsets3d = (np.array([path_w[i]]), np.array([path_b[i]]), np.array([path_J[i]]))\n","    trail2d.set_data(path_w[:i+1], path_b[:i+1])\n","    point2d.set_data([path_w[i]], [path_b[i]])\n","    dw, db = grad_full(path_w[i], path_b[i])\n","    info_text.set_text(f\"J(w,b) = {path_J[i]:.6f}   |   w = {path_w[i]:.4f}, b = {path_b[i]:.4f}   |   lr = {lr}\")\n","    grad_text.set_text(f\"dJ/dw = {dw:.4f}   dJ/db = {db:.4f}\")\n","    return trail3d, point3d, trail2d, point2d, info_text, grad_text\n","\n","ani = animation.FuncAnimation(fig, update, frames=len(path_w), init_func=init, interval=60, blit=False)\n","\n","html = ani.to_jshtml()\n","plt.close(fig)\n","display(HTML(html))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":735,"output_embedded_package_id":"12QrY4E8WJsOvxM4qxmEQxPyYnzWgOSZg"},"id":"Il3l2yBFFgi6","executionInfo":{"status":"ok","timestamp":1761834417356,"user_tz":300,"elapsed":71638,"user":{"displayName":"Gerardo Alexis Vilcamiza Espinoza","userId":"15529282681696955571"}},"outputId":"2ea2a6c3-85b6-47d2-b301-08fb3f38a81d"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}