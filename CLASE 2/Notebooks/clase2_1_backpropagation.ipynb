{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNmnx6eZ0T5v6Cv9/3xYZAk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Intuición: buscar el mínimo del costo\n","\n","Antes de entrar a la matemática del **Backpropagation**, pensemos en un caso muy simple: \"Una red con un solo parámetro $W$\".\n","\n","Nuestro objetivo es encontrar el valor de $W$ que minimiza la función de costo $C(W)$.\n","\n","Podemos imaginar a $C(W)$ como una colina con valles y cumbres.  \n","Lo que buscamos es el punto más bajo posible, el mínimo absoluto o, al menos, un buen mínimo local donde el costo (error) sea pequeño.\n","\n","A primera vista, parece sencillo. Si conocemos la expresión exacta de $C(W)$, basta con derivarla e igualar la derivada a cero para hallar los puntos críticos.  \n","Luego, aplicando la segunda derivada, podemos confirmar si el punto encontrado corresponde a un mínimo (concavidad hacia arriba) o a un máximo (concavidad hacia abajo).\n","\n","Por ejemplo, si la función de costo estuviera representada por:\n","\n","$$\n","C(W) = -0.07968W^5 + 0.48721W^4 + 0.28001W^3 - 4.65127W^2 + 15.7\n","$$\n","\n","entonces su primera derivada sería:\n","\n","$$\n","C'(W) = -0.3984W^4 + 1.94884W^3 + 0.84003W^2 - 9.30254W\n","$$\n","\n","y su segunda derivada:\n","\n","$$\n","C''(W) = -1.5936W^3 + 5.84652W^2 + 1.68006W - 9.30254\n","$$\n","\n","Con esas tres expresiones podríamos hallar analíticamente los puntos donde la función tiene máximos o mínimos y determinar la curvatura local para confirmar que se trata efectivamente de un mínimo. Tal cual se verá en la siguiente simulación:"],"metadata":{"id":"6-8DiRjGOvCE"}},{"cell_type":"code","source":["### SIMULACIÓN ###\n","\n","# El siguiente código es únicamente con fines de simular el ejemplo\n","\n","# Denle al botón \"play\" (►) de abajo para ver la animación\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","from IPython.display import HTML, display\n","\n","a5 = -0.07968\n","a4 =  0.48721\n","a3 =  0.28001\n","a2 = -4.65127\n","a1 =  0.0\n","a0 =  15.7\n","\n","def f(x):\n","    return a5*x**5 + a4*x**4 + a3*x**3 + a2*x**2 + a1*x + a0\n","\n","def fp(x):\n","    return 5*a5*x**4 + 4*a4*x**3 + 3*a3*x**2 + 2*a2*x + a1\n","\n","def fpp(x):\n","    return 20*a5*x**3 + 12*a4*x**2 + 6*a3*x + 2*a2\n","\n","xmin, xmax = -3.5, 4.0\n","X = np.linspace(xmin, xmax, 800)\n","Y = f(X)\n","\n","fig, ax = plt.subplots(figsize=(10, 6))\n","ax.plot(X, Y, linewidth=3, label='C(W)')\n","ax.axvline(3, linestyle='--', linewidth=1)\n","ax.axhline(1.5, linestyle=':', linewidth=1)\n","ax.scatter([3], [1.5], s=80)\n","ax.set_xlim(xmin, xmax)\n","ax.set_ylim(-5, 18)\n","ax.set_xlabel('W')\n","ax.set_ylabel('C(W)')\n","ax.grid(True)\n","\n","line_tan, = ax.plot([], [], color='black', linewidth=2.5, label='Tangente')\n","pt, = ax.plot([], [], 'ro', markersize=8)\n","info = ax.text(-3.3, 16.5, '', fontsize=11, bbox=dict(facecolor='white', alpha=0.75))\n","\n","def init():\n","    line_tan.set_data([], [])\n","    pt.set_data([], [])\n","    info.set_text('')\n","    return line_tan, pt, info\n","\n","def update(frame):\n","    x0 = -3.5 + frame * 0.05\n","    if x0 > 3.0:\n","        x0 = 3.0\n","    y0 = f(x0)\n","    m = fp(x0)\n","    c2 = fpp(x0)\n","    Yt = y0 + m*(X - x0)\n","    line_tan.set_data(X, Yt)\n","    pt.set_data([x0], [y0])\n","\n","    conc = \"∪ mínimo\" if c2 > 0 else \"∩ máximo\" if c2 < 0 else \"plano\"\n","    tol = 1e-3\n","    if abs(m) < tol:\n","        trend = \"estacionario\"\n","    elif m > 0:\n","        trend = \"aumenta\"\n","    else:\n","        trend = \"disminuye\"\n","\n","    ax.set_title(f'Tangente en W={x0:.2f}, C(W)={y0:.2f}')\n","    info.set_text(f\"C'(W)={m:.3f}\\nC''(W)={c2:.3f}\\nConcavidad: {conc}\\nError: {trend}\")\n","    return line_tan, pt, info\n","\n","frames = int((xmax - xmin) / 0.05)\n","ani = animation.FuncAnimation(fig, update, init_func=init, frames=frames, interval=30, blit=True)\n","\n","html = HTML(ani.to_jshtml())\n","display(html)\n","plt.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":697,"output_embedded_package_id":"1fH5MkiiSnQyFJ6mC8I7LDYUQrlt7LDFp"},"id":"RQY37WuaQfTr","executionInfo":{"status":"ok","timestamp":1761809408645,"user_tz":300,"elapsed":30701,"user":{"displayName":"Gerardo Alexis Vilcamiza Espinoza","userId":"15529282681696955571"}},"outputId":"f5cd2dbb-ed59-4436-ff58-256c12e5dd6d"},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["## ¿Y por qué esto no funciona con redes neuronales?\n","\n","El problema es que, en redes neuronales, el costo no depende de un solo parámetro, sino de **miles o millones de pesos y sesgos**:\n","\n","$$\n","C = f(W_1, W_2, W_3, \\dots, W_n)\n","$$\n","\n","A eso se suma que:\n","\n","1. La relación entre esos parámetros y el costo es **altamente no lineal**.  \n","2. La función resultante tiene una superficie llena de **mínimos locales, mesetas y regiones planas**.  \n","3. Y lo más importante: **no conocemos de forma explícita la expresión matemática exacta de $C$**.  \n","   Solo sabemos cómo calcular su valor numérico en la salida de la red tras un *forward pass*, pero no tenemos una ecuación cerrada que podamos derivar directamente.  \n","   En otras palabras, estamos \"a ciegas\" respecto a la forma completa de la función.\n","\n","Por eso, aunque la idea de usar derivadas no está equivocada, no podemos aplicarla de manera simbólica y directa como en el ejemplo anterior.  \n","En su lugar, necesitaremos un método que calcule las derivadas parciales de forma numérica y eficiente, incluso cuando la función es demasiado compleja para expresarse explícitamente.  \n","Ahí es donde entra el **Backpropagation**."],"metadata":{"id":"hRoDywBnU5Mb"}},{"cell_type":"markdown","source":["# Regla de la cadena en el backpropagation\n","\n","\n","## 1. Propagación hacia adelante\n","\n","Durante la *forward pass*, los valores fluyen desde la entrada hacia la salida:\n","\n","$$\n","z^{(L)} = W^{(L)} a^{(L-1)} + b^{(L)}\n","$$\n","\n","$$\n","a^{(L)} = f(z^{(L)})\n","$$\n","\n","donde:  \n","- $a^{(L-1)}$ son las activaciones de la capa anterior  \n","- $W^{(L)}$ y $b^{(L)}$ son los pesos y sesgos de la capa actual  \n","- $f$ es la función de activación (por ejemplo *sigmoid*, *ReLU*, *Tanh*, etc.)\n","\n","---\n","\n","## 2. Regla de la cadena\n","\n","El objetivo del *backpropagation* es calcular cómo cambia el error (costo) con respecto a cada parámetro de la red (peso o bias).\n","\n","Si una variable depende de otra, y esa depende de otra más, la regla de la cadena indica:\n","\n","$$\n","\\frac{dC}{dx} = \\frac{dC}{dy} \\cdot \\frac{dy}{dx}\n","$$\n","\n","Donde $y$ es una función que depende de $x$.\n","\n","<br>\n","\n","Y en cadenas más largas:\n","\n","$$\n","\\frac{dC}{dx} = \\frac{dC}{dz} \\cdot \\frac{dz}{dy} \\cdot \\frac{dy}{dx}\n","$$\n","\n","Donde $z$ es una función que depende de $y$, y $y$ es una función que depende de $x$.\n","\n","En redes neuronales, esto significa que el gradiente del error se propaga multiplicando derivadas capa a capa hacia atrás.\n"],"metadata":{"id":"PDtGAl3RLEJZ"}},{"cell_type":"code","source":["### SIMULACIÓN ###\n","\n","# El siguiente código es únicamente con fines de simular el ejemplo\n","\n","# Denle al botón \"play\" (►) de abajo para ver la animación\n","\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","from IPython.display import HTML\n","\n","matplotlib.rcParams[\"animation.html\"] = \"jshtml\"\n","matplotlib.rcParams[\"animation.embed_limit\"] = 25\n","\n","def sigmoid(x):\n","    return 1.0/(1.0+np.exp(-x))\n","\n","def chain_cost(x, t):\n","    y = sigmoid(x)\n","    z = y**2\n","    C = 0.5*(z - t)**2\n","    dC_dz = (z - t)\n","    dz_dy = 2.0*y\n","    dy_dx = y*(1.0-y)\n","    dC_dx = dC_dz*dz_dy*dy_dx\n","    return y, z, C, dC_dz, dz_dy, dy_dx, dC_dx\n","\n","def set_vline(vline, ax, x):\n","    vline.set_data([x, x], ax.get_ylim())\n","\n","t_target = 0.6\n","xgrid = np.linspace(-6, 6, 300)\n","ygrid = sigmoid(xgrid)\n","zgrid = ygrid**2\n","Cgrid = 0.5*(zgrid - t_target)**2\n","\n","fig, ax = plt.subplots(figsize=(10, 6))\n","line_y, = ax.plot(xgrid, ygrid, lw=2, label=\"y=σ(x)\")\n","line_z, = ax.plot(xgrid, zgrid, lw=2, ls=\"--\", label=\"z=y^2\")\n","line_C, = ax.plot(xgrid, Cgrid, lw=2, ls=\":\", label=\"C(x)\")\n","vline, = ax.plot([], [], lw=1)\n","dot_y, = ax.plot([], [], \"o\", ms=6)\n","dot_z, = ax.plot([], [], \"o\", ms=6)\n","dot_C, = ax.plot([], [], \"o\", ms=6)\n","txt = ax.text(0.02, 0.98, \"\", transform=ax.transAxes, va=\"top\")\n","ax.set_xlim(-6, 6)\n","ax.set_ylim(-0.1, max(Cgrid.max(), zgrid.max(), ygrid.max())+0.2)\n","ax.set_title(\"Regla de la cadena escalar\")\n","ax.set_xlabel(\"x\")\n","ax.set_ylabel(\"Valores\")\n","ax.legend(loc=\"lower right\")\n","\n","frames = 140\n","xs = np.linspace(-6, 6, frames)\n","\n","def init():\n","    set_vline(vline, ax, xs[0])\n","    dot_y.set_data([], [])\n","    dot_z.set_data([], [])\n","    dot_C.set_data([], [])\n","    txt.set_text(\"\")\n","    return line_y, line_z, line_C, vline, dot_y, dot_z, dot_C, txt\n","\n","def update(k):\n","    x = xs[k]\n","    y, z, C, dC_dz, dz_dy, dy_dx, dC_dx = chain_cost(x, t_target)\n","    set_vline(vline, ax, x)\n","    dot_y.set_data([x], [y])\n","    dot_z.set_data([x], [z])\n","    dot_C.set_data([x], [C])\n","    txt.set_text(\n","        f\"x={x:.3f}\\ny={y:.3f}\\nz={z:.3f}\\nC={C:.3f}\\n\"\n","        f\"∂C/∂z={dC_dz:.3f}\\n∂z/∂y={dz_dy:.3f}\\n∂y/∂x={dy_dx:.3f}\\n\"\n","        f\"∂C/∂x={dC_dx:.3f}\"\n","    )\n","    return line_y, line_z, line_C, vline, dot_y, dot_z, dot_C, txt\n","\n","anim = animation.FuncAnimation(fig, update, init_func=init, frames=frames, interval=40, blit=False)\n","html1 = HTML(anim.to_jshtml())\n","display(html1)\n","plt.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":697,"output_embedded_package_id":"1bJIzLFhAyrK5o2AcvmTx1oGOKJYxVl9h"},"id":"iX3Lt1BzXpmD","executionInfo":{"status":"ok","timestamp":1761809428811,"user_tz":300,"elapsed":20070,"user":{"displayName":"Gerardo Alexis Vilcamiza Espinoza","userId":"15529282681696955571"}},"outputId":"bc5123f8-d9c8-4225-9ff0-e4c12383aebe"},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["## 3. Propagación del error\n","\n","Sea $C$ la función de costo de la red. Buscamos hallar cómo varía este error con respecto a un parámetro de la red que puede ser un peso ($w$) o un bias ($b$).\n","\n","Entonces nuestro objetivo es hallar:\n","$$\n","\\frac{\\partial C}{\\partial w^{(L)}_{ij}}\n","$$\n","\n","Y también:\n","\n","$$\n","\\frac{\\partial C}{\\partial b^{(L)}_{i}}\n","$$\n","\n","Donde $w^{(L)}_{ij}$ es el peso que conecta la neurona número $j$ de la capa $L-1$ con la neurona número $i$ de la capa $L$.\n","\n","Y $b^{(L)}_{i}$ es el bias de la neurona numero $i$ de la capa $L$.\n","\n","<br>\n","\n","El problema es que no existe manera de hacer esas derivadas de manera directa, ya que existe un cadena de dependencias intermedias que no se puede ignorar.\n","\n","La función $C$ no depende directamente de $w$ o $b$, sino que depende de la salida de la función de activación $a^{(L)}$:\n","\n","$$\n","C = f(a^{(L)})\n","$$\n","\n","Esta a su vez, depende de la suma ponderada $z^{(L)}_{i}$:\n","\n","$$\n","a^{(L)} = f(z^{(L)})\n","$$\n","\n","Y esta última, recién es una función que depende de $w$ y $b$:\n","\n","$$\n","z^{(L)} = f(w, b)\n","$$\n","\n","Por lo que me queda lo siguiente:\n","\n","$$\n","C(a^{(L)}(z^{(L)}(w, b)))\n","$$\n","\n","<br>\n","\n","Debido a esto, la solución más óptima para calcular $\\frac{\\partial C}{\\partial w}$ y $\\frac{\\partial C}{\\partial b}$ es usando la regla de la cadena:\n","\n","$$\n","\\frac{\\partial C}{\\partial w^{(L)}} = \\frac{\\partial C}{\\partial a^{(L)}} \\cdot \\frac{\\partial a^{(L)}}{\\partial z^{(L)}} \\cdot \\frac{\\partial z^{(L)}}{\\partial w^{(L)}}\n","$$\n","\n","$$\n","\\frac{\\partial C}{\\partial b^{(L)}} = \\frac{\\partial C}{\\partial a^{(L)}} \\cdot \\frac{\\partial a^{(L)}}{\\partial z^{(L)}} \\cdot \\frac{\\partial z^{(L)}}{\\partial b^{(L)}}\n","$$\n","\n","---\n","\n","## 4. Vamos con las derivadas parciales\n","\n","Función de coste:\n","\n","$$\n","C(a^L_j) = \\frac{1}{2} \\sum_j (y_j - a^L_j)^2\n","$$\n","\n","Y su derivada con respecto a la activación de salida:\n","\n","$$\n","\\frac{\\partial C}{\\partial a^L_j} = (a^L_j - y_j)\n","$$\n","\n","<br>\n","\n","Función de activación: Sigmoide\n","\n","$$\n","a^L(z^L) = \\frac{1}{1 + e^{-z^L}}\n","$$\n","\n","Y su derivada con respecto a la entrada $z^L$:\n","\n","$$\n","\\frac{\\partial a^L}{\\partial z^L} = a^L(z^L) \\cdot (1 - a^L(z^L))\n","$$\n","\n","<br>\n","\n","Derivando la suma ponderada:\n","\n","$$\n","z^L(w, b) = \\sum_i a_i^{L-1} w_i^L + b^L\n","$$\n","\n","Derivadas parciales:\n","\n","$$\n","\\frac{\\partial z^{(L)}}{\\partial w_i^{(L)}} = a_i^{L-1}\n","$$\n","\n","$$\n","\\frac{\\partial z^{(L)}}{\\partial b^{(L)}} = 1\n","$$\n","\n","<br>\n","\n","Finalmente:\n","$$\n","\\frac{\\partial C}{\\partial W^{(L)}} = \\frac{\\partial C}{\\partial a^{(L)}} \\cdot \\frac{\\partial a^{(L)}}{\\partial z^{(L)}} \\cdot \\frac{\\partial z^{(L)}}{\\partial w^{(L)}} = (a^{(L)}_j - y_j) \\cdot a^{(L)}(z^{(L)}) \\cdot (1 - a^{(L)}(z^{(L)})) \\cdot a_i^{(L-1)}\n","$$\n","\n","$$\n","\\frac{\\partial C}{\\partial b^{(L)}} = \\frac{\\partial C}{\\partial a^{(L)}} \\cdot \\frac{\\partial a^{(L)}}{\\partial z^{(L)}} \\cdot \\frac{\\partial z^{(L)}}{\\partial b^{(L)}} = (a^{(L)}_j - y_j) \\cdot a^{(L)}(z^{(L)}) \\cdot (1 - a^{(L)}(z^{(L)})) \\cdot 1\n","$$\n","\n","<br>\n","\n","NOTA: Recordar que $a^{(L)}(z^{(L)})$ es una función y $a^{(L)}_j$ es un valor escalar de la salida de esa función.\n"],"metadata":{"id":"NgsSqRdFa9KG"}},{"cell_type":"code","source":["### SIMULACIÓN ###\n","\n","# El siguiente código es únicamente con fines de simular el ejemplo\n","\n","# Denle al botón \"play\" (►) de abajo para ver la animación\n","\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","from IPython.display import HTML, display\n","\n","matplotlib.rcParams[\"animation.html\"] = \"jshtml\"\n","matplotlib.rcParams[\"animation.embed_limit\"] = 25\n","\n","rng = np.random.default_rng(7)\n","m = 60\n","X = rng.normal(size=(m, 2))\n","true_w = np.array([[2.0, -1.0]])\n","true_b = np.array([0.5])\n","Y = 1.0/(1.0+np.exp(-(X @ true_w.T + true_b))) > 0.5\n","Y = Y.astype(float)\n","\n","W = rng.normal(scale=0.5, size=(1, 2))\n","b = np.array([0.0])\n","eta = 0.5\n","\n","def forward(X, W, b):\n","    Z = X @ W.T + b\n","    A = 1.0/(1.0+np.exp(-Z))\n","    return Z, A\n","\n","def loss(A, Y):\n","    return 0.5*np.mean((A - Y)**2)\n","\n","def grads(X, A, Y):\n","    m = X.shape[0]\n","    dC_dA = (A - Y)\n","    dA_dZ = A*(1.0-A)\n","    dC_dZ = dC_dA*dA_dZ\n","    dC_dW = (dC_dZ.T @ X)/m\n","    dC_db = dC_dZ.mean(axis=0)\n","    return dC_dW, dC_db, dC_dZ\n","\n","steps = 100\n","\n","fig, (ax_left, ax_right) = plt.subplots(1, 2, figsize=(13, 6), gridspec_kw={\"width_ratios\": [3, 3]})\n","\n","pos = Y.ravel() > 0.5\n","ax_left.scatter(X[pos,0], X[pos,1], s=20, label=\"Clase 1\")\n","ax_left.scatter(X[~pos,0], X[~pos,1], s=20, label=\"Clase 0\")\n","line_boundary, = ax_left.plot([], [], lw=2, label=\"Frontera\")\n","txt_left = ax_left.text(0.02, 0.98, \"\", transform=ax_left.transAxes, va=\"top\")\n","ax_left.set_title(\"Ultima capa: frontera y coste\")\n","ax_left.set_xlabel(\"x1\")\n","ax_left.set_ylabel(\"x2\")\n","ax_left.set_xlim(X[:,0].min()-1, X[:,0].max()+1)\n","ax_left.set_ylim(X[:,1].min()-1, X[:,1].max()+1)\n","ax_left.legend(loc=\"lower right\")\n","\n","ax_right.set_title(\"Costo vs época\")\n","ax_right.set_xlabel(\"Época\")\n","ax_right.set_ylabel(\"Costo\")\n","ax_right.set_xlim(0, steps)\n","ax_right.set_ylim(0.04, 0.12)\n","line_cost, = ax_right.plot([], [], lw=2)\n","dot_cost, = ax_right.plot([], [], \"o\", ms=6)\n","txt_right = ax_right.text(0.02, 0.98, \"\", transform=ax_right.transAxes, va=\"top\")\n","\n","cost_hist = []\n","\n","def init():\n","    line_boundary.set_data([], [])\n","    txt_left.set_text(\"\")\n","    line_cost.set_data([], [])\n","    dot_cost.set_data([], [])\n","    txt_right.set_text(\"\")\n","    return line_boundary, txt_left, line_cost, dot_cost, txt_right\n","\n","def update(t):\n","    global W, b\n","    Z, A = forward(X, W, b)\n","    C = loss(A, Y)\n","    dW, db, _ = grads(X, A, Y)\n","    W = W - eta*dW\n","    b = b - eta*db\n","    xx = np.linspace(ax_left.get_xlim()[0], ax_left.get_xlim()[1], 200)\n","    if abs(W[0,1]) < 1e-6:\n","        yy = np.full_like(xx, -b[0])\n","    else:\n","        yy = -(W[0,0]*xx + b[0])/W[0,1]\n","    line_boundary.set_data(xx, yy)\n","    txt_left.set_text(f\"W={W.ravel()}\\nb={b}\\nC={C:.5f}\")\n","\n","    cost_hist.append(float(C))\n","    xs = np.arange(len(cost_hist))\n","    ys = np.array(cost_hist)\n","    line_cost.set_data(xs, ys)\n","    if len(xs) > 0:\n","        dot_cost.set_data([xs[-1]], [ys[-1]])\n","    txt_right.set_text(f\"Época={t+1}\\nC={C:.5f}\")\n","\n","    return line_boundary, txt_left, line_cost, dot_cost, txt_right\n","\n","anim = animation.FuncAnimation(fig, update, init_func=init, frames=steps, interval=40, blit=False)\n","html2 = HTML(anim.to_jshtml())\n","display(html2)\n","plt.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":717,"output_embedded_package_id":"1M4rAb_PRFQH32Z-p10sutMv89u2KCegh"},"id":"_DbQBEX2aFSv","executionInfo":{"status":"ok","timestamp":1761809452415,"user_tz":300,"elapsed":23482,"user":{"displayName":"Gerardo Alexis Vilcamiza Espinoza","userId":"15529282681696955571"}},"outputId":"5c8f6089-8993-4c8c-ef5c-30697231896e"},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["## 5. Error imputado a una neurona\n","\n","Hemos visto cómo podemos saber cómo influye el valor de un parámetro en el error obtenido. ¿Pero cómo podemos saber por cuál de los caminos o ramificaciones de la red encontrar ese parámetro de manera más rápida?\n","\n","Para esto debemos encontrar la responsabilidad de cada neurona en el error hallando:\n","$$\n","\\delta^{(L)} = \\frac{\\partial C}{\\partial z^{(L)}} = \\frac{\\partial C}{\\partial a^{(L)}} \\cdot \\frac{\\partial a^{(L)}}{\\partial z^{(L)}} = (a^{(L)}_j - y_j) \\cdot a^{(L)}(z^{(L)}) \\cdot (1 - a^{(L)}(z^{(L)}))\n","$$\n","\n","Por lo que ahora para hallar la derivada del costo con respecto a los parametros sería:\n","\n","$$\n","\\frac{\\partial C}{\\partial W^{(L)}} = \\delta^{(L)} \\cdot a_i^{(L-1)}\n","$$\n","\n","$$\n","\\frac{\\partial C}{\\partial b^{(L)}} = \\delta^{(L)}\n","$$\n","\n","\n","---\n","\n","## 6. Propagación del error hacia capas anteriores\n","\n","En una capa oculta $L-1$, la neurona no \"ve\" el error directamente.  \n","Su salida solo afecta al error de la capa siguiente $L$.  \n","Por tanto, la variación del costo respecto a su entrada $z^{(L-1)}$ se obtiene a través de los pesos y errores de la capa siguiente:\n","\n","$$\n","\\delta^{(L-1)} = \\frac{\\partial C}{\\partial z^{(L-1)}}\n","$$\n","\n","Aplicando la regla de la cadena:\n","\n","$$\n","\\delta^{(L-1)} = \\frac{\\partial C}{\\partial z^{(L-1)}} = \\frac{\\partial C}{\\partial z^{(L)}_j} \\cdot \\frac{\\partial z^{(L)}_j}{\\partial a^{(L-1)}} \\cdot \\frac{\\partial a^{(L-1)}}{\\partial z^{(L-1)}}\n","$$\n","\n","Sabiendo que:\n","\n","$$\n","\\frac{\\partial z^{(L)}_j}{\\partial a^{(L-1)}_i} = w^{(L)}_{ji} = W^{(L)}\n","$$\n","\n","Y que:\n","\n","$$\n","\\frac{\\partial C}{\\partial z^{(L)}_j} = \\delta^{(L)}_j\n","$$\n","\n","Entonces:\n","\n","$$\n","\\delta^{(L-1)}_i = \\delta^{(L)}_j \\cdot W^{(L)} \\cdot \\frac{\\partial a^{(L-1)}}{\\partial z^{(L-1)}}\n","$$\n","\n","---\n","\n","## 7. Gradientes de pesos y bias en capas ocultas\n","\n","Una vez obtenido el error local de cada neurona $\\delta^{(L-1)}$, los gradientes se calculan de la misma forma que en la última capa:\n","\n","$$\n","\\frac{\\partial C}{\\partial W^{(L-1)}} = \\delta^{(L-1)} \\cdot a^{(L-2)}\n","$$\n","\n","$$\n","\\frac{\\partial C}{\\partial b^{(L-1)}} = \\delta^{(L-1)}\n","$$\n","\n","Este proceso se repite hacia atrás capa por capa hasta llegar a la primera."],"metadata":{"id":"8y_kT0koZQea"}},{"cell_type":"code","source":["### SIMULACIÓN ###\n","\n","# El siguiente código es únicamente con fines de simular el ejemplo\n","\n","# Denle al botón \"play\" (►) de abajo para ver la animación\n","\n","# Simulación para una red 2-2-1. Es decir, 2 neuronas en la capa de entrada, una capa oculta de 2 neuronas y 1 neurona en la capa de salida.\n","\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","from IPython.display import HTML, display\n","\n","matplotlib.rcParams[\"animation.html\"] = \"jshtml\"\n","matplotlib.rcParams[\"animation.embed_limit\"] = 25\n","\n","rng = np.random.default_rng(3)\n","X = rng.normal(size=(80, 2))\n","\n","w1 = np.array([[1.2, -0.7],\n","               [-0.9,  1.0]], dtype=float)\n","b1 = np.array([0.0, 0.0], dtype=float)\n","w2 = np.array([[0.8, -0.6]], dtype=float)\n","b2 = np.array([0.0], dtype=float)\n","\n","Y = (X[:,0]**2 + X[:,1]**2 > 1.3).astype(float).reshape(-1,1)\n","\n","def sigmoid(x):\n","    return 1.0/(1.0+np.exp(-x))\n","\n","def forward_2_2_1(X, w1, b1, w2, b2):\n","    z1 = X @ w1.T + b1\n","    a1 = sigmoid(z1)\n","    z2 = a1 @ w2.T + b2\n","    a2 = sigmoid(z2)\n","    return z1, a1, z2, a2\n","\n","def loss_mse(a2, Y):\n","    return 0.5*np.mean((a2 - Y)**2)\n","\n","def backprop_2_2_1(X, Y, z1, a1, z2, a2, w2):\n","    m = X.shape[0]\n","    dC_da2 = (a2 - Y)\n","    da2_dz2 = a2*(1.0-a2)\n","    delta2 = dC_da2*da2_dz2\n","    dC_dw2 = (delta2.T @ a1)/m\n","    dC_db2 = delta2.mean(axis=0)\n","    da1_dz1 = a1*(1.0-a1)\n","    delta1 = (delta2 @ w2)*da1_dz1\n","    dC_dw1 = (delta1.T @ X)/m\n","    dC_db1 = delta1.mean(axis=0)\n","    return delta1, delta2, dC_dw1, dC_db1, dC_dw2, dC_db2\n","\n","eta = 1.0\n","steps3 = 220\n","steps_per_frame = 15\n","\n","xlim = (X[:,0].min()-1, X[:,0].max()+1)\n","ylim = (X[:,1].min()-1, X[:,1].max()+1)\n","\n","grid_x = np.linspace(xlim[0], xlim[1], 200)\n","grid_y = np.linspace(ylim[0], ylim[1], 200)\n","GX, GY = np.meshgrid(grid_x, grid_y)\n","G = np.stack([GX.ravel(), GY.ravel()], axis=1)\n","\n","fig, (ax_dec, ax_cost) = plt.subplots(1, 2, figsize=(13, 6), gridspec_kw={\"width_ratios\": [3, 3]})\n","\n","pos = Y.ravel() > 0.5\n","Z0 = sigmoid(sigmoid(G @ w1.T + b1) @ w2.T + b2).reshape(GX.shape)\n","img = ax_dec.imshow(Z0, extent=[grid_x.min(), grid_x.max(), grid_y.min(), grid_y.max()],\n","                    origin=\"lower\", alpha=0.6, vmin=0.0, vmax=1.0, interpolation=\"nearest\")\n","s1 = ax_dec.scatter(X[pos,0], X[pos,1], s=16, label=\"Clase 1\")\n","s0 = ax_dec.scatter(X[~pos,0], X[~pos,1], s=16, label=\"Clase 0\")\n","cs = ax_dec.contour(GX, GY, Z0, levels=[0.5], colors=\"k\", linewidths=2)\n","txt = ax_dec.text(0.02, 0.98, \"\", transform=ax_dec.transAxes, va=\"top\")\n","ax_dec.set_title(\"Backprop 2-2-1: evolución de la frontera p=0.5\")\n","ax_dec.set_xlabel(\"x1\")\n","ax_dec.set_ylabel(\"x2\")\n","ax_dec.set_xlim(*xlim)\n","ax_dec.set_ylim(*ylim)\n","ax_dec.legend(loc=\"lower right\")\n","\n","ax_cost.set_title(\"Costo vs época\")\n","ax_cost.set_xlabel(\"Época\")\n","ax_cost.set_ylabel(\"Costo\")\n","ax_cost.set_xlim(0, steps3)\n","ax_cost.set_ylim(0.05, 0.13)\n","line_cost, = ax_cost.plot([], [], lw=2)\n","dot_cost, = ax_cost.plot([], [], \"o\", ms=6)\n","cost_hist = []\n","\n","def init3():\n","    line_cost.set_data([], [])\n","    dot_cost.set_data([], [])\n","    txt.set_text(\"\")\n","    return img, s1, s0, txt, line_cost, dot_cost\n","\n","def update3(t):\n","    global w1, b1, w2, b2\n","    C_show = None\n","    for _ in range(steps_per_frame):\n","        z1, a1, z2, a2 = forward_2_2_1(X, w1, b1, w2, b2)\n","        C = loss_mse(a2, Y)\n","        delta1, delta2, dC_dw1, dC_db1, dC_dw2, dC_db2 = backprop_2_2_1(X, Y, z1, a1, z2, a2, w2)\n","        w2 -= eta*dC_dw2\n","        b2 -= eta*dC_db2\n","        w1 -= eta*dC_dw1\n","        b1 -= eta*dC_db1\n","        C_show = C\n","    cost_hist.append(float(C_show))\n","\n","    Zg = sigmoid(sigmoid(G @ w1.T + b1) @ w2.T + b2).reshape(GX.shape)\n","    ax_dec.cla()\n","    ax_dec.imshow(Zg, extent=[grid_x.min(), grid_x.max(), grid_y.min(), grid_y.max()],\n","                  origin=\"lower\", alpha=0.6, vmin=0.0, vmax=1.0, interpolation=\"nearest\")\n","    ax_dec.scatter(X[pos,0], X[pos,1], s=16, label=\"Clase 1\")\n","    ax_dec.scatter(X[~pos,0], X[~pos,1], s=16, label=\"Clase 0\")\n","    ax_dec.contour(GX, GY, Zg, levels=[0.5], colors=\"k\", linewidths=2)\n","    ax_dec.set_title(\"Backprop 2-2-1: evolución de la frontera p=0.5\")\n","    ax_dec.set_xlim(*xlim)\n","    ax_dec.set_ylim(*ylim)\n","    ax_dec.legend(loc=\"lower right\")\n","    ax_dec.text(0.02, 0.98, f\"Época={t+1}\\nC={C_show:.4f}\", transform=ax_dec.transAxes, va=\"top\")\n","\n","    xs = np.arange(1, len(cost_hist)+1)\n","    ys = np.array(cost_hist)\n","    line_cost.set_data(xs, ys)\n","    dot_cost.set_data([xs[-1]], [ys[-1]])\n","\n","    return line_cost, dot_cost\n","\n","anim3 = animation.FuncAnimation(fig, update3, init_func=init3, frames=steps3, interval=50, blit=False)\n","html3 = HTML(anim3.to_jshtml())\n","display(html3)\n","plt.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":735,"output_embedded_package_id":"1RWSRWV8Ga_IjlDcqOtirzLp-2NIQ5cF1"},"id":"RjILbTptbQue","executionInfo":{"status":"ok","timestamp":1761809534340,"user_tz":300,"elapsed":81683,"user":{"displayName":"Gerardo Alexis Vilcamiza Espinoza","userId":"15529282681696955571"}},"outputId":"1568f02d-8e9d-458f-ab23-7e87d37f1f3d"},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}